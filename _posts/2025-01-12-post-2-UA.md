---
title: "Про один з способів реалізації RAID1 спільного сховища високої доступності (HA) для кластера Proxmox з двох вузлів з їх дисків."
categories:
  - Blog
tags:
  - Shared Volume
  - HA
  - Live Migration 
  - RAID
  - Proxmox
  - nvme
  - scsi
  - multipath
---
# 2. Про один з способів реалізації RAID1 спільного сховища високої доступності (HA) для кластера Proxmox з двох вузлів з їх дисків.

**Наступне рішення, на мій погляд, просте і дешеве в реалізації.**

**Полягає в наступному:**  

  1.	Є два вузла з nvme дисками (не обов'язково). На кожному встановлюються пакет: "nvme-cli" ("nvme-tcp", транслюємо диски в мережу з кожного вузла, можна скористатися і icsi пакета "tgt" ) та пакет: "open-iscsi", пакет: "multipath-tools" (як один з способів, можна використовувати для покращення продуктивності, можна bond, коли є можливість зв'язати вузли декількома мережевими з'єднаннями).  
  2.	Між цими вузлами встановлюється мережа.     
  3.	На одному з вузлів робимо віртуальну машину "VSAN105" на основі Linux приблизно таких характеристик RAM 512Mb, HDD 3Gb та інсталюємо пакети:  "mdadm" (збудуємо RAID1 масив з відповідних розділів nvme-дисків вузлів: nvme discavery... , connect... , та потім підтримуємо), "tgt" (для трансляції цілі md0 iscsi на два вузли), "lvm2" (з отриманого  масиву md0 робимо pvcreate, vgcreate).  
  4.	Налагоджуємо "open-iscsi" на вузлах.  
  5.	Запускаємо VSAN. Вона транслює iscsi md0 з відповідною VG. 
  6.	В дата центрі Proxmox робимо спільне сховище LVM.  
  7.	Робимо з цієї віртуальної машини таку, що працює в оперативній пам'яті без використання диска. Тепер вона може працювати в режимі живої міграції і підтримувати спільне сховище.  
  8.	Переводимо віртуальну машину VSAN в HA.  
  9.	Оповіщення, діагностика, вирішення проблем RAID лягає на утиліти пакета "mdadm".

## 1.	Підготовка вузлів. 
### 1.1. Є два вузла з nvme дисками. На кожному встановлюються пакет: "nvme-cli", та пакет: "open-iscsi", пакет: "multipath-tools":
```
apt update 
apt install nvme-cli open-iscsi multipath-tools
```
### 1.2. Транслюємо диски в мережу з кожного вузла:  

**На вузлі pve1 10.10.1.1**  

**Зробимо виконуючий файл:**  
```
nano  /root/nvme_start.sh
#!/bin/bash
modprobe nvmet
modprobe nvmet-tcp
# Налаштування порта
mkdir /sys/kernel/config/nvmet/ports/1
cd /sys/kernel/config/nvmet/ports/1
echo 10.10.1.1 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.1.1 tcp ipv4"
mkdir /sys/kernel/config/nvmet/ports/2
cd /sys/kernel/config/nvmet/ports/2
echo 10.10.2.1 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.2.1 tcp ipv4"
name_nvme_disk=$(udevadm info --query=name /dev/disk/by-path/pci-0000:02:00.0-nvme-1)
echo "$name_nvme_disk"
        cd /sys/kernel/config/nvmet/subsystems
        mkdir ora10
        cd ora10
        echo "oral0"
        echo -n 1 > attr_allow_any_host
        nvme list | grep -w $name_nvme_disk | awk '{print "pve1-"$3}' > attr_serial
        nvme list | grep -w $name_nvme_disk | awk '{print "pve1-"$4}' > attr_model
        cd namespaces/
        mkdir "10"; cd "10"
        echo -n "/dev/${name_nvme_disk}" > device_path; echo -n 1 > enable;
        # Кожну підсистему, опісля конфигурації, приєднуємо до порту
        sleep 5
        ln -s /sys/kernel/config/nvmet/subsystems/ora10 /sys/kernel/config/nvmet/ports/1/subsystems/ora10
        ln -s /sys/kernel/config/nvmet/subsystems/ora10 /sys/kernel/config/nvmet/ports/2/subsystems/ora10
exit 0
```
```
root@pve1:~# ./root/nvme_start.sh
root@pve1:~# apt install tree

root@pve1:~#  tree /sys/kernel/config/nvmet/ 
/sys/kernel/config/nvmet/
├── hosts
├── ports
│   ├── 1
│   │   ├── addr_adrfam
│   │   ├── addr_traddr
│   │   ├── addr_treq
│   │   ├── addr_trsvcid
│   │   ├── addr_trtype
│   │   ├── addr_tsas
│   │   ├── ana_groups
│   │   │   └── 1
│   │   │       └── ana_state
│   │   ├── param_inline_data_size
│   │   ├── param_pi_enable
│   │   ├── referrals
│   │   └── subsystems
│   │       └── ora10 -> ../../../../nvmet/subsystems/ora10
│   └── 2
│       ├── addr_adrfam
│       ├── addr_traddr
│       ├── addr_treq
│       ├── addr_trsvcid
│       ├── addr_trtype
│       ├── addr_tsas
│       ├── ana_groups
│       │   └── 1
│       │       └── ana_state
│       ├── param_inline_data_size
│       ├── param_pi_enable
│       ├── referrals
│       └── subsystems
│           └── ora10 -> ../../../../nvmet/subsystems/ora10
└── subsystems
    └── ora10
        ├── allowed_hosts
        ├── attr_allow_any_host
        ├── attr_cntlid_max
        ├── attr_cntlid_min
        ├── attr_firmware
        ├── attr_ieee_oui
        ├── attr_model
        ├── attr_pi_enable
        ├── attr_qid_max
        ├── attr_serial
        ├── attr_version
        ├── namespaces
        │   └── 10
        │       ├── ana_grpid
        │       ├── buffered_io
        │       ├── device_nguid
        │       ├── device_path
        │       ├── device_uuid
        │       ├── enable
        │       ├── p2pmem
        │       └── revalidate_size
        └── passthru
            ├── admin_timeout
            ├── clear_ids
            ├── device_path
            ├── enable
            └── io_timeout

21 directories, 41 files
root@pve1:~# 
```
**На вузлі pve99  10.10.1.2**  

**Зробимо аналогічний виконуючий файл:**  
```
nano  /root/nvme_start.sh 
#!/bin/bash
modprobe nvmet
modprobe nvmet-tcp
# # Налаштування порта
mkdir /sys/kernel/config/nvmet/ports/1
cd /sys/kernel/config/nvmet/ports/1
echo 10.10.1.2 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.1.2 tcp ipv4"
mkdir /sys/kernel/config/nvmet/ports/2
cd /sys/kernel/config/nvmet/ports/2
echo 10.10.2.2 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.2.2 tcp ipv4"
name_nvme_disk=$(udevadm info --query=name /dev/disk/by-path/pci-0000:05:00.0-nvme-1)
echo "$name_nvme_disk"
        cd /sys/kernel/config/nvmet/subsystems
        mkdir ora20
        cd ora20
        echo "ora20"
        echo -n 1 > attr_allow_any_host
        nvme list | grep -w $name_nvme_disk | awk '{print "pve99-"$3}' > attr_serial
        nvme list | grep -w $name_nvme_disk | awk '{print "pve99-"$4}' > attr_model
        cd namespaces/
        mkdir "20"; cd "20"
        echo -n "/dev/${name_nvme_disk}" > device_path; echo -n 1 > enable;
        # Кожну підсистему, опісля конфигурації, приєднуємо до порту
        sleep 5
        ln -s /sys/kernel/config/nvmet/subsystems/ora20 /sys/kernel/config/nvmet/ports/1/subsystems/ora20
        ln -s /sys/kernel/config/nvmet/subsystems/ora20 /sys/kernel/config/nvmet/ports/2/subsystems/ora20
exit 0
```
```
[root@pve99 ~]$ ./root/nvme_start.sh
[root@pve99 ~]$ apt install tree

[root@pve99 ~]$ tree /sys/kernel/config/nvmet/
/sys/kernel/config/nvmet/
├── hosts
├── ports
│   ├── 1
│   │   ├── addr_adrfam
│   │   ├── addr_traddr
│   │   ├── addr_treq
│   │   ├── addr_trsvcid
│   │   ├── addr_trtype
│   │   ├── addr_tsas
│   │   ├── ana_groups
│   │   │   └── 1
│   │   │       └── ana_state
│   │   ├── param_inline_data_size
│   │   ├── param_pi_enable
│   │   ├── referrals
│   │   └── subsystems
│   │       └── ora20 -> ../../../../nvmet/subsystems/ora20
│   └── 2
│       ├── addr_adrfam
│       ├── addr_traddr
│       ├── addr_treq
│       ├── addr_trsvcid
│       ├── addr_trtype
│       ├── addr_tsas
│       ├── ana_groups
│       │   └── 1
│       │       └── ana_state
│       ├── param_inline_data_size
│       ├── param_pi_enable
│       ├── referrals
│       └── subsystems
│           └── ora20 -> ../../../../nvmet/subsystems/ora20
└── subsystems
    └── ora20
        ├── allowed_hosts
        ├── attr_allow_any_host
        ├── attr_cntlid_max
        ├── attr_cntlid_min
        ├── attr_firmware
        ├── attr_ieee_oui
        ├── attr_model
        ├── attr_pi_enable
        ├── attr_qid_max
        ├── attr_serial
        ├── attr_version
        ├── namespaces
        │   └── 20
        │       ├── ana_grpid
        │       ├── buffered_io
        │       ├── device_nguid
        │       ├── device_path
        │       ├── device_uuid
        │       ├── enable
        │       ├── p2pmem
        │       └── revalidate_size
        └── passthru
            ├── admin_timeout
            ├── clear_ids
            ├── device_path
            ├── enable
            └── io_timeout

21 directories, 41 files
[root@pve99 ~]$ 
```
## 2.	Між цими вузлами встановлюється мережа. 
В данному випадку два порти вузлів з'єднанні напряму  

## 3.	На одному з вузлів робимо віртуальну машину "VSAN105" (10.10.1.3) на основі Linux таких характеристик (512Mb, 3Gb) та інсталюємо пакети:  "mdadm" (збудуємо RAID1 масив з відповідних розділів nvme-дисків вузлів: nvme discavery... , connect... , та потім підтримуємо), "tgt" (для трансляції цілі md0 iscsi на два вузли), "lvm2" (з отриманого  масиву md0 робимо pvcreate, vgcreate).

### 3.1. Підготовка віртуальної машини до використання у якості забезпечувача сховища кластера.

### 3.2. Доберемо Debian в якості операційної системи.
**Встановлюємо**
`apt install tgt`

`nano /etc/tgt/conf.d/tgtpve.conf` 
**Наповнення:**  
```
<target iqn.1993-08.org.debian:01:9e746ebec3e>
   backing-store /dev/md0
   initiator-address 10.10.1.1
   initiator-address 10.10.1.2
</target>
```

### 3.3. Встановлюємо "lvm2": 
`apt install lvm2`

### 3.4. Встановлюємо "nvme-cli": 
`apt install nvme-cli`

**Завантаження модулів при старті системи:**  

`modprobe nvme_tcp && echo "nvme_tcp" > /etc/modules-load.d/nvme_tcp.conf`  

**Підключитися:**  
```
nvme discover -t tcp -a 10.10.1.1 -s 4420
nvme connect -t tcp -n ora10 -a 10.10.1.1 -s 4420
nvme discover -t tcp -a 10.10.1.2 -s 4420
nvme connect -t tcp -n ora20 -a 10.10.1.2 -s 4420
```
**Щоб підключатися при старті:**

`nano /etc/nvme/discovery.conf`  

**Вставимо:**  
```
# Used for extracting default parameters for discovery
#
# Example:
# --transport=<trtype> --traddr=<traddr> --trsvcid=<trsvcid> --host-traddr=<host-traddr> --host-iface=<host-iface>
discover -t tcp -a 10.10.1.1 -s 4420
discover -t tcp -a 10.10.1.2 -s 4420
```
**Потім:**
```
systemctl enable nvmf-autoconnect.service
```
### 3.5. Підготовка дисків (зміна розміру сектора за потреби) 

**Нода 1**  
```
root@pve1:~# nvme list
Node Generic SN Model Namespace Usage Format FW Rev
/dev/nvme1n1 /dev/ng1n1 S4EUNG0M328258D Samsung SSD 970 EVO Plus 250GB 1 214.99 GB / 250.06 GB 512 B + 0 B 1B2QEXM7
/dev/nvme0n1 /dev/ng0n1 50026B7282A726A4 KINGSTON SKC3000S512G 1 512.11 GB / 512.11 GB 4 KiB + 0 B EIFK31.6
```
**Первіряємо розмір блоку**  
```
root@pve1:~# nvme id-ns /dev/nvme0 -n 1 -H | grep &quot;LBA Format&quot;
[6:5] : 0 Most significant 2 bits of Current LBA Format Selected
[3:0] : 0x1 Least significant 4 bits of Current LBA Format Selected
LBA Format 0 : Metadata Size: 0 bytes - Data Size: 512 bytes - Relative Performance: 0x2 Good
LBA Format 1 : Metadata Size: 0 bytes - Data Size: 4096 bytes - Relative Performance: 0x1 Better (in use)
root@pve1:~#
```
**За потреби, змінюємо на 4k**  
``` 
root@pve1:~# nvme id-ns /dev/format --lbaf=1 /dev/nvme0n1
```
### 3.6. Розмічаємо диск.
```
root@pve1:~# fdisk /dev/nvme0n1
```
**Зберігаємо розмітку, щоб скористатися, коли замінюємо диск і зараз для дубля:**  
```
root@pve1:~# sfdisk -d /dev/nvme0n1 > nvmeKINGSTON512.dump
root@pve1:~# cat nvmeKINGSTON512.dump
label: gpt
label-id: A1F37274-73E6-864F-B0B6-9BDD551BBD45
device: /dev/nvme0n1
unit: sectors
first-lba: 256
last-lba: 125026896
sector-size: 4096
/dev/nvme0n1p1 : start= 4096, size= 262144, type=0657FD6D-A4AB-43C4-84E5-0933C84B4F4F, uuid=B21C1B97-64EE-6948-AA0F-0BBA8797EB91
/dev/nvme0n1p2 : start= 266240, size= 104857600, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4, uuid=E71CF74F-B553-5246-A649-3A5C45619225
/dev/nvme0n1p3 : start= 105123840, size= 16777216, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4, uuid=47A78E4D-FC8E-F54B-8DAF-D52A7908590C
/dev/nvme0n1p4 : start= 121901056, size= 3125760, type=0657FD6D-A4AB-43C4-84E5-0933C84B4F4F, uuid=7F23E207-9E1A-1B42-962F-98BED3C1F479
```
**Щоб пізніше відновити цей шаблон, ви можете виконати:**  
```
# sfdisk /dev/nvme0n1 < nvmeKINGSTON512.dump
root@pve1:~#
```
**Другий диск розбиваємо аналогічно**  
```
[root@pve99 ~]$ sfdisk /dev/nvme0n1 < nvmeKINGSTON512.dump
[root@pve99 ~]$ sfdisk -d /dev/nvme0n1 > nvmeKINGSTON512_pve99.dump
[root@pve99 ~]$ cat nvmeKINGSTON512_pve99.dump
label: gpt
label-id: BB0E5F37-05C1-104A-8CB5-6D4A7DA9073A
device: /dev/nvme0n1
unit: sectors
first-lba: 256
last-lba: 125026896
sector-size: 4096
/dev/nvme0n1p1 : start= 4096, size= 262144, type=C12A7328-F81F-11D2-BA4B-00A0C93EC93B, uuid=B139BB2A-C95D-1641-968F-3ED977E40B6A
/dev/nvme0n1p2 : start= 266240, size= 104857600, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4, uuid=3CF0D8B4-A0C3-1145-B923-05D59A56B406
/dev/nvme0n1p3 : start= 105123840, size= 16777216, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4, uuid=35726BB3-0E33-C440-B5A1-14F31EE527FD
/dev/nvme0n1p4 : start= 121901056, size= 3125760, type=0657FD6D-A4AB-43C4-84E5-0933C84B4F4F, uuid=4512150A-2074-5D44-90E3-58826E7A6152
[root@pve99 ~]$
```
## Далі переходимо до віртуальної машини:

### 3.7. Інсталюємо mdadm:
`apt install mdadm`

### 3.8. Налагоджуємо Raid1 пристроїв */dev/nvme1n1p2 , /dev/nvme2n1p2*:  
```  
nvme-list
```

`root@debvsan:/home/vov# nvme list`    

```
|   Node     |  Generic    |           SN            |      Model        |  Namespace Usage    |         Format           |       FW      |   Rev     |
|-----------:|:-----------:|:-----------------------:|:-----------------:|:-------------------:|:------------------------:|:-------------:|:----------|
|/dev/nvme2n1|  /dev/ng2n1 |  9782709cba71d57d8e6b   |   pve99-KINGSTON  |           20        |  512.11  GB / 512.11  GB |  4 KiB +  0 B |  6.8.12-5 |
|/dev/nvme1n1|  /dev/ng1n1 |  f1f5c6929bb211d228f4   |   pve1-KINGSTON   |           10        |  512.11  GB / 512.11  GB |  4 KiB +  0 B |  6.8.12-5 |

```
```
root@debvsan:/home/vov#
mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/nvme1n1p2 /dev/nvme2n1p2
```
**Налаштуємо "mdadm" повторне збирання масиву під час перезавантаження, а потім оновимо initrd, щоб дозволити "mdadm" там залишитися.**  
```
mdadm --detail --scan | tee -a /etc/mdadm/mdadm.conf
update-initramfs –u
```
## 4. Налогоджуємо "open-iscsi" на вузлах.

## 4.1. Встановлюємо покети на кожному з вузлів:
`apt install open-iscsi multipath-tools`

**Запускаємо сервіс:**  

`systemctl start open-iscsi.service`  

### 4.2. На першому вузлі під'єднуємось до віртуальної машини, ми знаходимо ціль і входимо в систему:
`iscsiadm -m discovery -t st -p 10.10.1.3`

### 4.3. На другому вузлі:  
```
[root@pve99:~iscsiadm -m discovery -t st -p 10.10.1.3
10.10.1.3:3260,1 iqn.1993-08.org.debian:01:9e746ebec3e
```

## 5. На віртуальній машині робимо:  
```
# pvcreate /dev/md0
# vgcreate vg_nvme1 /dev/md0
```
**На вузлах і в віртуальній машині команда: vgs дає відображення групи: vg_nvme1**  

## 6. В графічній оболонці Datacenter Storage LVM, ставимо позначку Shared і обираємо відповідно вузли.

## 7. Робимо віртуальну машину, щоб працювала тільки в оперативній пам'яті без диска. Ставимо її в HA. Система буде працездатною до відмови обох вузлів, обох дисків, чи декількох каналів мережі.

### 7.1. У файлі /usr/share/initramfs-tools/scripts/local шукаємо на рядках 179-185 (попередньо зробивши бекап файлу):  
```
   checkfs "${ROOT}" root "${FSTYPE}"

	# Mount root
	# shellcheck disable=SC2086
	if ! mount ${roflag} ${FSTYPE:+-t "${FSTYPE}"} ${ROOTFLAGS} "${ROOT}" "${rootmnt?}"; then
		panic "Failed to mount ${ROOT} as root file system."
	fi
```
І **змінюємо цей код на такий:**  
```
   #checkfs "${ROOT}" root "${FSTYPE}"

	# Mount root
	# shellcheck disable=SC2086
	mkdir /ramboottmp
	mount ${roflag} -t ${FSTYPE} ${ROOTFLAGS} ${ROOT} /ramboottmp
	mount -t tmpfs -o size=100% none ${rootmnt}
	cd ${rootmnt}
	cp -rfa /ramboottmp/* ${rootmnt}
	umount /ramboottmp
```
### 7.2. Зберігаємо файл. І вводимо команду в терміналі від root:  

`mkinitramfs -o /boot/initrd.img-ramboot`  

### 7.3. Перевіряємо, що файл створено в папці /boot і повертаємо старий local у папці /usr/share/initramfs-tools/scripts/local на місце (або видаляємо всі наші зміни, які ми зробили за крок 1).  

### 7.4. Ідемо в папку: /etc і знаходимо файл: fstab, зберігаємо його копію і редагуємо його, шукаємо в перших рядках щось на зразок цього:  

`UUID= 321dba83-9a22-442b-b06b-185d7afe1088 / ext4 defaults 1 1`

**і міняємо на:**  

`none / tmpfs defaults 0 0` 

### 7.5 Зробимо відповідне меню при завантаженні:
**файл:**
```
nano /etc/grub.d/40_custom
```
**Вміст:**
```
#!/bin/sh
exec tail -n +3 $0
# This file provides an easy way to add custom menu entries.  Simply type the
# menu entries you want to add after this comment.  Be careful not to change
# the 'exec tail' line above.
menuentry 'RAM-Debian GNU/Linux' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-simple-321dba83-9a22-442b-b06b-185d7afe1088' {
        load_video
        insmod gzio
        insmod part_msdos
        insmod ext2
        set root='hd0,msdos1'
        echo    'Loading Linux 6.1.0-28-amd64 ...'
        linux   /boot/vmlinuz-6.1.0-28-amd64 root=UUID=321dba83-9a22-442b-b06b-185d7afe1088 ro  quiet splash toram
        echo    'Loading initial ramdisk ...'
        initrd  /boot/initrd.img-ramboot
}
```
```
update-grub
```
**Отримуємо меню grub.**

### 7.5. Тепер зробимо ram.tar.gz, вимкнувши віртуальну машину, завантажуємо нову віртуальну машину в режимі liveCD, підключивши диск цієї віртуальної машини.  

**Змонтуємо його в /mnt. Виконаємо:**  
```
# cd /mnt
# tar -czf /mnt/boot/ram.tar.gz .
```
### 7.6. Тепер при загрузці віртуальної машини, вибираємо відповідне меню завантаження в RAM. Після завантаження відімкнемо диск:  

#### 7.6.1. Зупинимо доступ до диска:  

**Для цього використовується команда:**  
```
echo 1 > /sys/block/sda/device/delete`
```
     
**Це відключить пристрій /dev/sda на рівні ядра. Диск більше не буде видимий у системі.**  

#### 7.6.2. Перевіримо стан:  

**Переконаймося, що диск більше не з’являється у списку пристроїв:**  
```
lsblk
```

**Виглядатиме десь так:**  
```
root@debvsan:/home/vov# lsblk
NAME                            MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINTS
sda                               8:0    0     8G  0 disk  
└─sda1                            8:1    0     8G  0 part  
root@debvsan:/home/vov#
```
```
echo 1 > /sys/block/sda/device/delete
```
```
lsblk
```
#### 7.6.3. Від’єднання диска від віртуальної машини через QEMU монітор:  

##### 7.6.3.1. Увійдіть у монітор QEMU для конкретної VM:  
```
qm monitor 105
```

**Перевірте всі пристрої, підключені до VM:**  
```
info block  
```
**Це покаже всі підключені диски. Ви побачите щось подібне:**  
```
root@pve1:~# qm monitor 105 
Entering QEMU Monitor for VM 105 - type 'help' for help
qm> info block
drive-scsi0 (#block190): /dev/pve/vm-105-disk-1 (raw)
    Attached to:      scsi0
    Cache mode:       writeback, direct
    Detect zeroes:    unmap
qm>
```

**Від’єднайте диск:**
```
device_del scsi0
```

**Перевірте, які пристрої підключені:**
```
info pci
```

**Ви побачите список PCI-пристроїв, включаючи SCSI-контролер.   
Наприклад:**  
```
Bus  9, device   1, function 0:
    SCSI controller: PCI device 1af4:1004
      PCI subsystem 1af4:0008
      IRQ 10, pin A
      BAR0: I/O at 0x1000 [0x103f].
      BAR1: 32 bit memory at 0xfd800000 [0xfd800fff].
      BAR4: 64 bit prefetchable memory at 0xfc000000 [0xfc003fff].
      id "virtioscsi0"
```
**Видалити:**  
```
qm> device_del virtioscsi0
```
**Вийти:**  
```
q
```

##### 7.3.4.2. Вставити до конфігураційного файлу:  
```
root@pve1:~# nano /etc/pve/qemu-server/105.conf
```
**Наступне: disabled=1  
Приклад:**  
```
scsi0: local-lvm:vm-105-disk-1,disabled=1,aio=native,backup=0,discard=on,iothread=1,size=8G
scsihw: virtio-scsi-single,disabled=1
```
##### 7.3.4.3. І При міграції ми побачимо:  
```
()
Task viewer: VM 105 - Migrate
OutputStatus
Stop
Download
task started by HA resource agent
2025-01-04 00:34:24 use dedicated network address for sending migration traffic (10.10.1.1)
2025-01-04 00:34:24 starting migration of VM 105 to node 'pve1' (10.10.1.1)
2025-01-04 00:34:24 starting VM 105 on remote node 'pve1'
2025-01-04 00:34:28 start remote tunnel
2025-01-04 00:34:29 ssh tunnel ver 1
2025-01-04 00:34:29 starting online/live migration on unix:/run/qemu-server/105.migrate
2025-01-04 00:34:29 set migration capabilities
2025-01-04 00:34:29 migration downtime limit: 100 ms
2025-01-04 00:34:29 migration cachesize: 512.0 MiB
2025-01-04 00:34:29 set migration parameters
2025-01-04 00:34:29 start migrate command to unix:/run/qemu-server/105.migrate
2025-01-04 00:34:30 migration active, transferred 357.9 MiB of 4.0 GiB VM-state, 586.6 MiB/s
2025-01-04 00:34:31 migration active, transferred 735.9 MiB of 4.0 GiB VM-state, 543.8 MiB/s
2025-01-04 00:34:32 migration active, transferred 1.1 GiB of 4.0 GiB VM-state, 399.1 MiB/s
2025-01-04 00:34:33 migration active, transferred 1.3 GiB of 4.0 GiB VM-state, 502.5 MiB/s
2025-01-04 00:34:34 migration active, transferred 1.6 GiB of 4.0 GiB VM-state, 243.0 MiB/s
2025-01-04 00:34:35 migration active, transferred 1.8 GiB of 4.0 GiB VM-state, 320.5 MiB/s
2025-01-04 00:34:36 migration active, transferred 2.2 GiB of 4.0 GiB VM-state, 462.0 MiB/s
2025-01-04 00:34:37 migration active, transferred 2.5 GiB of 4.0 GiB VM-state, 443.7 MiB/s
2025-01-04 00:34:38 average migration speed: 457.0 MiB/s - downtime 73 ms
2025-01-04 00:34:38 migration status: completed
2025-01-04 00:34:42 migration finished successfully (duration 00:00:18)
TASK OK
```
### 8. Переводимо віртуальну машину в Dtacenter в HA.
Задопомогою графічного інтерфейсу Proxmox переводимо vm 105 в HA.
### 9. Оповіщення, діагностика, вирішення проблем RAID1 лягає на утиліти пакета "mdadm".

**Далі буде**
