---
title: "3.	Про один з способів реалізації RAID1 спільного сховища для кластера Proxmox з двох вузлів з їх дисків."
categories:
  - Blog
tags:
  - DRBD 
  - RDMA
  - RAID1
  - Proxmox
---
# 3. Про один з способів реалізації RAID1 спільного сховища для кластера Proxmox з двох вузлів з їх дисків.

**Наступне рішення, на мій погляд, просте і дешеве в реалізації і піді йде для домашнього кластера чи для кластера організації.**
{: .notice--info}  

**Рішення полягає в наступному:**  

  1.	Є два вузла з nvme дисками. На кожному встановлюються пакет: nvme-cli (nvme-tcp, транслюємо диски в мережу з кожного вузла) та пакет: open-iscsi, пакет: multipath-tools (можна використовувати для покращення продуктивності, коли є можливість зв'язати вузли декількома мережевими з'єднаннями).  
  
  2.	Між цими вузлами встановлюється мережа.   
  
  3.	На одному з вузлів робимо віртуальну машину VSAN на основі Linux приблизно таких характеристик (4096Mb, 8Gb) та інсталюємо пакети:  mdadm (збудуємо RAID1 масив з відповідних розділів nvme-дисків вузлів: nvme discavery... , connect... , та потім підтримуємо), tgt (для трансляції цілі md0 iscsi на два вузли), lvm2 (з отриманого  масиву md0 робимо pvcreate, vgcreate), multipath-tools (можна використовувати для покращення продуктивності, коли є можливість зв'язати вузли декількома мережевими з'єднаннями).   
  
  4.	Налагоджуємо open-iscsi на вузлах.  
  
  5.	Запускаємо VSAN. Вона транслює iscsi md0 з відповідною VG.  
  
  6.	В дата центрі Proxmox робимо спільне сховище (iscsi, lvm).  
  
  7.	Робимо з цієї віртуальної машини таку, що працює в оперативній пам'яті без використання диска. Тепер вона може працювати в режимі живої міграції і підтримувати спільне сховище.  
  
  8.	Переводимо віртуальну машину VSAN в HA.  
  
  9.	Оповіщення, діагностика, вирішення проблем RAID1 лягає на утиліти пакета mdadm.

## 1.	Підготовка вузлів. 
### 1.1. Є два вузла з nvme дисками. На кожному встановлюються пакет: nvme-cli, та пакет: open-iscsi, пакет: multipath-tools:
```
apt update 
apt install nvme-cli open-iscsi multipath-tools
```
### 1.2. Транслюємо диски в мережу з кожного вузла:  

**На вузлі pve1 10.10.1.1**  

**Зробимо виконуючий файл:**  
```
nano  /root/nvme_start.sh
#!/bin/bash
modprobe nvmet
modprobe nvmet-tcp
# Настройка порта без изменений
mkdir /sys/kernel/config/nvmet/ports/1
cd /sys/kernel/config/nvmet/ports/1
echo 10.10.1.1 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.1.1 tcp ipv4"

mkdir /sys/kernel/config/nvmet/ports/2
cd /sys/kernel/config/nvmet/ports/2
echo 10.10.2.1 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.2.1 tcp ipv4"


name_nvme_disk=$(udevadm info --query=name /dev/disk/by-path/pci-0000:02:00.0-nvme-1)
echo "$name_nvme_disk"

        cd /sys/kernel/config/nvmet/subsystems

        mkdir ora10

        cd ora10
        echo "oral0" 

        echo -n 1 > attr_allow_any_host
        nvme list | grep -w $name_nvme_disk | awk '{print "pve1-"$3}' > attr_serial
        nvme list | grep -w $name_nvme_disk | awk '{print "pve1-"$4}' > attr_model
        cd namespaces/
        mkdir "10"; cd "10"
        echo -n "/dev/${name_nvme_disk}" > device_path; echo -n 1 > enable;
        # Каждую подсистему, после конфигурации, присоединяем к порту
        sleep 5
        ln -s /sys/kernel/config/nvmet/subsystems/ora10 /sys/kernel/config/nvmet/ports/1/subsystems/ora10
        ln -s /sys/kernel/config/nvmet/subsystems/ora10 /sys/kernel/config/nvmet/ports/2/subsystems/ora10




exit 0
```
```
root@pve1:~# apt install tree

root@pve1:~#  tree /sys/kernel/config/nvmet/ 
/sys/kernel/config/nvmet/
├── hosts
├── ports
│   ├── 1
│   │   ├── addr_adrfam
│   │   ├── addr_traddr
│   │   ├── addr_treq
│   │   ├── addr_trsvcid
│   │   ├── addr_trtype
│   │   ├── addr_tsas
│   │   ├── ana_groups
│   │   │   └── 1
│   │   │       └── ana_state
│   │   ├── param_inline_data_size
│   │   ├── param_pi_enable
│   │   ├── referrals
│   │   └── subsystems
│   │       └── ora10 -> ../../../../nvmet/subsystems/ora10
│   └── 2
│       ├── addr_adrfam
│       ├── addr_traddr
│       ├── addr_treq
│       ├── addr_trsvcid
│       ├── addr_trtype
│       ├── addr_tsas
│       ├── ana_groups
│       │   └── 1
│       │       └── ana_state
│       ├── param_inline_data_size
│       ├── param_pi_enable
│       ├── referrals
│       └── subsystems
│           └── ora10 -> ../../../../nvmet/subsystems/ora10
└── subsystems
    └── ora10
        ├── allowed_hosts
        ├── attr_allow_any_host
        ├── attr_cntlid_max
        ├── attr_cntlid_min
        ├── attr_firmware
        ├── attr_ieee_oui
        ├── attr_model
        ├── attr_pi_enable
        ├── attr_qid_max
        ├── attr_serial
        ├── attr_version
        ├── namespaces
        │   └── 10
        │       ├── ana_grpid
        │       ├── buffered_io
        │       ├── device_nguid
        │       ├── device_path
        │       ├── device_uuid
        │       ├── enable
        │       ├── p2pmem
        │       └── revalidate_size
        └── passthru
            ├── admin_timeout
            ├── clear_ids
            ├── device_path
            ├── enable
            └── io_timeout

21 directories, 41 files
root@pve1:~# 
```
**На вузлі pve99  10.10.1.2**  

**Зробимо аналогічний виконуючий файл:**  
```
nano  /root/nvme_start.sh 
#!/bin/bash
modprobe nvmet
modprobe nvmet-tcp
# Настройка порта без изменений
mkdir /sys/kernel/config/nvmet/ports/1
cd /sys/kernel/config/nvmet/ports/1
echo 10.10.1.2 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.1.2 tcp ipv4"

mkdir /sys/kernel/config/nvmet/ports/2
cd /sys/kernel/config/nvmet/ports/2
echo 10.10.2.2 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.2.2 tcp ipv4"

name_nvme_disk=$(udevadm info --query=name /dev/disk/by-path/pci-0000:05:00.0-nvme-1)
echo "$name_nvme_disk"

        cd /sys/kernel/config/nvmet/subsystems

        mkdir ora20

        cd ora20
        echo "ora20" 

        echo -n 1 > attr_allow_any_host
        nvme list | grep -w $name_nvme_disk | awk '{print "pve99-"$3}' > attr_serial
        nvme list | grep -w $name_nvme_disk | awk '{print "pve99-"$4}' > attr_model
        cd namespaces/
        mkdir "20"; cd "20"
        echo -n "/dev/${name_nvme_disk}" > device_path; echo -n 1 > enable;
        # Каждую подсистему, после конфигурации, присоединяем к порту
        sleep 5
        ln -s /sys/kernel/config/nvmet/subsystems/ora20 /sys/kernel/config/nvmet/ports/1/subsystems/ora20
        ln -s /sys/kernel/config/nvmet/subsystems/ora20 /sys/kernel/config/nvmet/ports/2/subsystems/ora20

exit 0
```
```
[root@pve99 ~]$ tree /sys/kernel/config/nvmet/
/sys/kernel/config/nvmet/
├── hosts
├── ports
│   ├── 1
│   │   ├── addr_adrfam
│   │   ├── addr_traddr
│   │   ├── addr_treq
│   │   ├── addr_trsvcid
│   │   ├── addr_trtype
│   │   ├── addr_tsas
│   │   ├── ana_groups
│   │   │   └── 1
│   │   │       └── ana_state
│   │   ├── param_inline_data_size
│   │   ├── param_pi_enable
│   │   ├── referrals
│   │   └── subsystems
│   │       └── ora20 -> ../../../../nvmet/subsystems/ora20
│   └── 2
│       ├── addr_adrfam
│       ├── addr_traddr
│       ├── addr_treq
│       ├── addr_trsvcid
│       ├── addr_trtype
│       ├── addr_tsas
│       ├── ana_groups
│       │   └── 1
│       │       └── ana_state
│       ├── param_inline_data_size
│       ├── param_pi_enable
│       ├── referrals
│       └── subsystems
│           └── ora20 -> ../../../../nvmet/subsystems/ora20
└── subsystems
    └── ora20
        ├── allowed_hosts
        ├── attr_allow_any_host
        ├── attr_cntlid_max
        ├── attr_cntlid_min
        ├── attr_firmware
        ├── attr_ieee_oui
        ├── attr_model
        ├── attr_pi_enable
        ├── attr_qid_max
        ├── attr_serial
        ├── attr_version
        ├── namespaces
        │   └── 20
        │       ├── ana_grpid
        │       ├── buffered_io
        │       ├── device_nguid
        │       ├── device_path
        │       ├── device_uuid
        │       ├── enable
        │       ├── p2pmem
        │       └── revalidate_size
        └── passthru
            ├── admin_timeout
            ├── clear_ids
            ├── device_path
            ├── enable
            └── io_timeout

21 directories, 41 files
[root@pve99 ~]$ 
```
## 2.	Між цими вузлами встановлюється мережа.  

## 3.	На одному з вузлів робимо віртуальну машину VSAN на основі Linux таких характеристик (4096Mb, 8Gb) та інсталюємо пакети:  mdadm (збудуємо RAID1 масив з відповідних розділів nvme-дисків вузлів: nvme discavery... , connect... , та потім підтримуємо), tgt (для трансляції цілі md0 iscsi на два вузли), lvm2 (з отриманого  масиву md0 робимо pvcreate, vgcreate), multipath-tools (можна використовувати для покращення продуктивності, коли є можливість зв'язати вузли декількома мережевими з'єднаннями).  

### 3.1. Підготовка віртуальної машини до використання у якості забезпечувача сховища кластера.  

### 3.2. Доберемо Debian в якості операційної системи.Встановлюємо apt install tgt.  

`nano /etc/tgt/conf.d/tgtpve.conf`  

**Наповнення:**  
```
<target iqn.1993-08.org.debian:01:9e746ebec3e>
   backing-store /dev/md0
   initiator-address 10.10.1.1
   initiator-address 10.10.1.2
</target>
```

### 3.3. Встановлюємо lvm2: apt install lvm2  

### 3.4. apt install nvme-cli  

**Завантаження модулів при старті системи:**  

`modprobe nvme_tcp && echo "nvme_tcp" > /etc/modules-load.d/nvme_tcp.conf`  

**Підключитися:**  
```
nvme discover -t tcp -a 10.10.1.1 -s 4420
nvme connect -t tcp -n ora10 -a 10.10.1.1 -s 4420
nvme discover -t tcp -a 10.10.1.2 -s 4420
nvme connect -t tcp -n ora20 -a 10.10.1.2 -s 4420
```
**Щоб підключатися при старті:**
{: .notice--success}

`nano /etc/nvme/discovery.conf`  

**Вставимо:**  
```
# Used for extracting default parameters for discovery
#
# Example:
# --transport=<trtype> --traddr=<traddr> --trsvcid=<trsvcid> --host-traddr=<host-traddr> --host-iface=<host-iface>
discover -t tcp -a 10.10.1.1 -s 4420
discover -t tcp -a 10.10.1.2 -s 4420
Потім:
systemctl enable nvmf-autoconnect.service
```
