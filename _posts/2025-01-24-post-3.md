---
title: "3. Про один з способів реалізації бездискової віртуальної машини на основі Linux для кластерів, яка працює тільки в RAM."
categories:
  - Blog
tags:
  - VM completely into RAM
  - RAM VM
  - Diskless VM
  - HA
  - Live Migration 
  - Proxmox
---
# 3. Про один з способів реалізації бездискової віртуальної машини на основі Linux для кластерів, яка працює тільки в RAM.

**Наступне рішення, на мій погляд, гарне для реалізаціїї віртуальної машини, яка не використовує сховище, може працювати врежимі HA, живої міграції і виконувати різноманітні задачі і забеспечувати відмовостійкість, зроблено на Linux Debian в кластері Proxmox.**

## 1. Робимо віртуальну машину, щоб працювала тільки в оперативній пам'яті без диска. Ставимо її в HA.
### 1.1. Робимо віртуальну машину Linux стандартним шляхом і налаштовуємо відповідно задачам, що будуть на її покладені. Диск розташовуємо на сховищі спільному чи локальному. Цей диск буде використовуватися для першого старту. А потім машина буде від'єднана від цього диску і може "кочувати" по вузлах. 
### 1.2. У файлі /usr/share/initramfs-tools/scripts/local шукаємо на рядках 179-185 (попередньо зробивши бекап файлу):  
```
   checkfs "${ROOT}" root "${FSTYPE}"

	# Mount root
	# shellcheck disable=SC2086
	if ! mount ${roflag} ${FSTYPE:+-t "${FSTYPE}"} ${ROOTFLAGS} "${ROOT}" "${rootmnt?}"; then
		panic "Failed to mount ${ROOT} as root file system."
	fi
```
І **змінюємо цей код на такий:**  
```
   #checkfs "${ROOT}" root "${FSTYPE}"

	# Mount root
	# shellcheck disable=SC2086
	mkdir /ramboottmp
	mount ${roflag} -t ${FSTYPE} ${ROOTFLAGS} ${ROOT} /ramboottmp
	mount -t tmpfs -o size=100% none ${rootmnt}
	cd ${rootmnt}
	cp -rfa /ramboottmp/* ${rootmnt}
	umount /ramboottmp
```
### 1.3. Зберігаємо файл. І вводимо команду в терміналі від root:  

`mkinitramfs -o /boot/initrd.img-ramboot`  

### 1.4. Перевіряємо, що файл створено в папці /boot і повертаємо старий local у папці /usr/share/initramfs-tools/scripts/local на місце (або видаляємо всі наші зміни, які ми зробили за крок 1).  

### 1.5. Ідемо в папку: /etc і знаходимо файл: fstab, зберігаємо його копію і редагуємо його, шукаємо в перших рядках щось на зразок цього:  

`UUID= 321dba83-9a22-442b-b06b-185d7afe1088 / ext4 defaults 1 1`

**і міняємо на:**  

`none / tmpfs defaults 0 0` 

### 1.6 Зробимо відповідне меню при завантаженні:
**файл:**
```
nano /etc/grub.d/40_custom
```
**Вміст:**
```
#!/bin/sh
exec tail -n +3 $0
# This file provides an easy way to add custom menu entries.  Simply type the
# menu entries you want to add after this comment.  Be careful not to change
# the 'exec tail' line above.
menuentry 'RAM-Debian GNU/Linux' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-simple-321dba83-9a22-442b-b06b-185d7afe1088' {
        load_video
        insmod gzio
        insmod part_msdos
        insmod ext2
        set root='hd0,msdos1'
        echo    'Loading Linux 6.1.0-28-amd64 ...'
        linux   /boot/vmlinuz-6.1.0-28-amd64 root=UUID=321dba83-9a22-442b-b06b-185d7afe1088 ro  quiet splash toram
        echo    'Loading initial ramdisk ...'
        initrd  /boot/initrd.img-ramboot
}
```
```
update-grub
```
**Отримуємо меню grub.**

### 1.6. Тепер зробимо ram.tar.gz, вимкнувши віртуальну машину, завантажуємо нову віртуальну машину в режимі liveCD, підключивши диск цієї віртуальної машини.  

**Змонтуємо його в /mnt. Виконаємо:**  
```
# cd /mnt
# tar -czf /mnt/boot/ram.tar.gz .
```
### 1.7. Тепер при загрузці віртуальної машини, вибираємо відповідне меню завантаження в RAM. Після завантаження відімкнемо диск:  

#### 1.7.1. Зупинимо доступ до диска:  

**Для цього використовується команда:**  
```
echo 1 > /sys/block/sda/device/delete
```
     
**Це відключить пристрій /dev/sda на рівні ядра. Диск більше не буде видимий у системі.**  

#### 1.7.2. Перевіримо стан:  

**Переконаймося, що диск більше не з’являється у списку пристроїв:**  
```
lsblk
```

**Виглядатиме десь так:**  
```
root@debvsan:/home/vov# lsblk
NAME                            MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINTS
sda                               8:0    0     8G  0 disk  
└─sda1                            8:1    0     8G  0 part  
root@debvsan:/home/vov#
```
```
echo 1 > /sys/block/sda/device/delete
```
```
lsblk
```
#### 1.7.3. Від’єднання диска від віртуальної машини через QEMU монітор:  

##### 1.7.3.1. Увійдіть у монітор QEMU для конкретної VM:  
```
qm monitor 105
```

**Перевірте всі пристрої, підключені до VM:**  
```
info block  
```
**Це покаже всі підключені диски. Ви побачите щось подібне:**  
```
root@pve1:~# qm monitor 105 
Entering QEMU Monitor for VM 105 - type 'help' for help
qm> info block
drive-scsi0 (#block190): /dev/pve/vm-105-disk-1 (raw)
    Attached to:      scsi0
    Cache mode:       writeback, direct
    Detect zeroes:    unmap
qm>
```

**Від’єднайте диск:**
```
device_del scsi0
```

**Перевірте, які пристрої підключені:**
```
info pci
```

**Ви побачите список PCI-пристроїв, включаючи SCSI-контролер.   
Наприклад:**  
```
Bus  9, device   1, function 0:
    SCSI controller: PCI device 1af4:1004
      PCI subsystem 1af4:0008
      IRQ 10, pin A
      BAR0: I/O at 0x1000 [0x103f].
      BAR1: 32 bit memory at 0xfd800000 [0xfd800fff].
      BAR4: 64 bit prefetchable memory at 0xfc000000 [0xfc003fff].
      id "virtioscsi0"
```
**Видалити:**  
```
qm> device_del virtioscsi0
```
**Вийти:**  
```
q
```

##### 1.7.4.2. Вставити до конфігураційного файлу:  
```
root@pve1:~# nano /etc/pve/qemu-server/105.conf
```
**Наступне:**
```
disabled=1
```
**Приклад:**  
```
scsi0: local-lvm:vm-105-disk-1,disabled=1,aio=native,backup=0,discard=on,iothread=1,size=8G
scsihw: virtio-scsi-single,disabled=1
```
##### 1.7.4.3. І При міграції ми побачимо:  
```
()
Task viewer: VM 105 - Migrate
OutputStatus
Stop
Download
task started by HA resource agent
2025-01-04 00:34:24 use dedicated network address for sending migration traffic (10.10.1.1)
2025-01-04 00:34:24 starting migration of VM 105 to node 'pve1' (10.10.1.1)
2025-01-04 00:34:24 starting VM 105 on remote node 'pve1'
2025-01-04 00:34:28 start remote tunnel
2025-01-04 00:34:29 ssh tunnel ver 1
2025-01-04 00:34:29 starting online/live migration on unix:/run/qemu-server/105.migrate
2025-01-04 00:34:29 set migration capabilities
2025-01-04 00:34:29 migration downtime limit: 100 ms
2025-01-04 00:34:29 migration cachesize: 512.0 MiB
2025-01-04 00:34:29 set migration parameters
2025-01-04 00:34:29 start migrate command to unix:/run/qemu-server/105.migrate
2025-01-04 00:34:30 migration active, transferred 357.9 MiB of 4.0 GiB VM-state, 586.6 MiB/s
2025-01-04 00:34:31 migration active, transferred 735.9 MiB of 4.0 GiB VM-state, 543.8 MiB/s
2025-01-04 00:34:32 migration active, transferred 1.1 GiB of 4.0 GiB VM-state, 399.1 MiB/s
2025-01-04 00:34:33 migration active, transferred 1.3 GiB of 4.0 GiB VM-state, 502.5 MiB/s
2025-01-04 00:34:34 migration active, transferred 1.6 GiB of 4.0 GiB VM-state, 243.0 MiB/s
2025-01-04 00:34:35 migration active, transferred 1.8 GiB of 4.0 GiB VM-state, 320.5 MiB/s
2025-01-04 00:34:36 migration active, transferred 2.2 GiB of 4.0 GiB VM-state, 462.0 MiB/s
2025-01-04 00:34:37 migration active, transferred 2.5 GiB of 4.0 GiB VM-state, 443.7 MiB/s
2025-01-04 00:34:38 average migration speed: 457.0 MiB/s - downtime 73 ms
2025-01-04 00:34:38 migration status: completed
2025-01-04 00:34:42 migration finished successfully (duration 00:00:18)
TASK OK
```

