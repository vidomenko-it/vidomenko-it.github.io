---
title: "3.	Про один з способів реалізації RAID1 спільного сховища для кластера Proxmox з двох вузлів з їх дисків."
categories:
  - Blog
tags:
  - DRBD 
  - RDMA
  - RAID1
  - Proxmox
---
# 3. Про один з способів реалізації RAID1 спільного сховища для кластера Proxmox з двох вузлів з їх дисків.

**Наступне рішення, на мій погляд, просте і дешеве в реалізації і піді йде для домашнього кластера чи для кластера організації.**
{: .notice--info}  

**Рішення полягає в наступному:**  

  1.	Є два вузла з nvme дисками. На кожному встановлюються пакет: nvme-cli (nvme-tcp, транслюємо диски в мережу з кожного вузла) та пакет: open-iscsi, пакет: multipath-tools (можна використовувати для покращення продуктивності, коли є можливість зв'язати вузли декількома мережевими з'єднаннями).  
  
  2.	Між цими вузлами встановлюється мережа.   
  
  3.	На одному з вузлів робимо віртуальну машину VSAN на основі Linux приблизно таких характеристик (4096Mb, 8Gb) та інсталюємо пакети:  mdadm (збудуємо RAID1 масив з відповідних розділів nvme-дисків вузлів: nvme discavery... , connect... , та потім підтримуємо), tgt (для трансляції цілі md0 iscsi на два вузли), lvm2 (з отриманого  масиву md0 робимо pvcreate, vgcreate), multipath-tools (можна використовувати для покращення продуктивності, коли є можливість зв'язати вузли декількома мережевими з'єднаннями).   
  
  4.	Налагоджуємо open-iscsi на вузлах.  
  
  5.	Запускаємо VSAN. Вона транслює iscsi md0 з відповідною VG.  
  
  6.	В дата центрі Proxmox робимо спільне сховище (iscsi, lvm).  
  
  7.	Робимо з цієї віртуальної машини таку, що працює в оперативній пам'яті без використання диска. Тепер вона може працювати в режимі живої міграції і підтримувати спільне сховище.  
  
  8.	Переводимо віртуальну машину VSAN в HA.  
  
  9.	Оповіщення, діагностика, вирішення проблем RAID1 лягає на утиліти пакета mdadm.

## 1.	Підготовка вузлів. 
### 1.1. Є два вузла з nvme дисками. На кожному встановлюються пакет: nvme-cli, та пакет: open-iscsi, пакет: multipath-tools:
```
apt update 
apt install nvme-cli open-iscsi multipath-tools
```
### 1.2. Транслюємо диски в мережу з кожного вузла:  

**На вузлі pve1 10.10.1.1**  

**Зробимо виконуючий файл:**  
```
nano  /root/nvme_start.sh
#!/bin/bash
modprobe nvmet
modprobe nvmet-tcp
# Настройка порта без изменений
mkdir /sys/kernel/config/nvmet/ports/1
cd /sys/kernel/config/nvmet/ports/1
echo 10.10.1.1 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.1.1 tcp ipv4"

mkdir /sys/kernel/config/nvmet/ports/2
cd /sys/kernel/config/nvmet/ports/2
echo 10.10.2.1 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.2.1 tcp ipv4"


name_nvme_disk=$(udevadm info --query=name /dev/disk/by-path/pci-0000:02:00.0-nvme-1)
echo "$name_nvme_disk"

        cd /sys/kernel/config/nvmet/subsystems

        mkdir ora10

        cd ora10
        echo "oral0" 

        echo -n 1 > attr_allow_any_host
        nvme list | grep -w $name_nvme_disk | awk '{print "pve1-"$3}' > attr_serial
        nvme list | grep -w $name_nvme_disk | awk '{print "pve1-"$4}' > attr_model
        cd namespaces/
        mkdir "10"; cd "10"
        echo -n "/dev/${name_nvme_disk}" > device_path; echo -n 1 > enable;
        # Каждую подсистему, после конфигурации, присоединяем к порту
        sleep 5
        ln -s /sys/kernel/config/nvmet/subsystems/ora10 /sys/kernel/config/nvmet/ports/1/subsystems/ora10
        ln -s /sys/kernel/config/nvmet/subsystems/ora10 /sys/kernel/config/nvmet/ports/2/subsystems/ora10




exit 0
```
```
root@pve1:~# apt install tree

root@pve1:~#  tree /sys/kernel/config/nvmet/ 
/sys/kernel/config/nvmet/
├── hosts
├── ports
│   ├── 1
│   │   ├── addr_adrfam
│   │   ├── addr_traddr
│   │   ├── addr_treq
│   │   ├── addr_trsvcid
│   │   ├── addr_trtype
│   │   ├── addr_tsas
│   │   ├── ana_groups
│   │   │   └── 1
│   │   │       └── ana_state
│   │   ├── param_inline_data_size
│   │   ├── param_pi_enable
│   │   ├── referrals
│   │   └── subsystems
│   │       └── ora10 -> ../../../../nvmet/subsystems/ora10
│   └── 2
│       ├── addr_adrfam
│       ├── addr_traddr
│       ├── addr_treq
│       ├── addr_trsvcid
│       ├── addr_trtype
│       ├── addr_tsas
│       ├── ana_groups
│       │   └── 1
│       │       └── ana_state
│       ├── param_inline_data_size
│       ├── param_pi_enable
│       ├── referrals
│       └── subsystems
│           └── ora10 -> ../../../../nvmet/subsystems/ora10
└── subsystems
    └── ora10
        ├── allowed_hosts
        ├── attr_allow_any_host
        ├── attr_cntlid_max
        ├── attr_cntlid_min
        ├── attr_firmware
        ├── attr_ieee_oui
        ├── attr_model
        ├── attr_pi_enable
        ├── attr_qid_max
        ├── attr_serial
        ├── attr_version
        ├── namespaces
        │   └── 10
        │       ├── ana_grpid
        │       ├── buffered_io
        │       ├── device_nguid
        │       ├── device_path
        │       ├── device_uuid
        │       ├── enable
        │       ├── p2pmem
        │       └── revalidate_size
        └── passthru
            ├── admin_timeout
            ├── clear_ids
            ├── device_path
            ├── enable
            └── io_timeout

21 directories, 41 files
root@pve1:~# 
```
**На вузлі pve99  10.10.1.2**  

**Зробимо аналогічний виконуючий файл:**  
```
nano  /root/nvme_start.sh 
#!/bin/bash
modprobe nvmet
modprobe nvmet-tcp
# Настройка порта без изменений
mkdir /sys/kernel/config/nvmet/ports/1
cd /sys/kernel/config/nvmet/ports/1
echo 10.10.1.2 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.1.2 tcp ipv4"

mkdir /sys/kernel/config/nvmet/ports/2
cd /sys/kernel/config/nvmet/ports/2
echo 10.10.2.2 | tee -a addr_traddr > /dev/null
echo tcp | tee -a addr_trtype > /dev/null
echo 4420 | tee -a addr_trsvcid > /dev/null
echo ipv4 | tee -a addr_adrfam > /dev/null
echo "port 4420 IP 10.10.2.2 tcp ipv4"

name_nvme_disk=$(udevadm info --query=name /dev/disk/by-path/pci-0000:05:00.0-nvme-1)
echo "$name_nvme_disk"

        cd /sys/kernel/config/nvmet/subsystems

        mkdir ora20

        cd ora20
        echo "ora20" 

        echo -n 1 > attr_allow_any_host
        nvme list | grep -w $name_nvme_disk | awk '{print "pve99-"$3}' > attr_serial
        nvme list | grep -w $name_nvme_disk | awk '{print "pve99-"$4}' > attr_model
        cd namespaces/
        mkdir "20"; cd "20"
        echo -n "/dev/${name_nvme_disk}" > device_path; echo -n 1 > enable;
        # Каждую подсистему, после конфигурации, присоединяем к порту
        sleep 5
        ln -s /sys/kernel/config/nvmet/subsystems/ora20 /sys/kernel/config/nvmet/ports/1/subsystems/ora20
        ln -s /sys/kernel/config/nvmet/subsystems/ora20 /sys/kernel/config/nvmet/ports/2/subsystems/ora20

exit 0
```
```
[root@pve99 ~]$ tree /sys/kernel/config/nvmet/
/sys/kernel/config/nvmet/
├── hosts
├── ports
│   ├── 1
│   │   ├── addr_adrfam
│   │   ├── addr_traddr
│   │   ├── addr_treq
│   │   ├── addr_trsvcid
│   │   ├── addr_trtype
│   │   ├── addr_tsas
│   │   ├── ana_groups
│   │   │   └── 1
│   │   │       └── ana_state
│   │   ├── param_inline_data_size
│   │   ├── param_pi_enable
│   │   ├── referrals
│   │   └── subsystems
│   │       └── ora20 -> ../../../../nvmet/subsystems/ora20
│   └── 2
│       ├── addr_adrfam
│       ├── addr_traddr
│       ├── addr_treq
│       ├── addr_trsvcid
│       ├── addr_trtype
│       ├── addr_tsas
│       ├── ana_groups
│       │   └── 1
│       │       └── ana_state
│       ├── param_inline_data_size
│       ├── param_pi_enable
│       ├── referrals
│       └── subsystems
│           └── ora20 -> ../../../../nvmet/subsystems/ora20
└── subsystems
    └── ora20
        ├── allowed_hosts
        ├── attr_allow_any_host
        ├── attr_cntlid_max
        ├── attr_cntlid_min
        ├── attr_firmware
        ├── attr_ieee_oui
        ├── attr_model
        ├── attr_pi_enable
        ├── attr_qid_max
        ├── attr_serial
        ├── attr_version
        ├── namespaces
        │   └── 20
        │       ├── ana_grpid
        │       ├── buffered_io
        │       ├── device_nguid
        │       ├── device_path
        │       ├── device_uuid
        │       ├── enable
        │       ├── p2pmem
        │       └── revalidate_size
        └── passthru
            ├── admin_timeout
            ├── clear_ids
            ├── device_path
            ├── enable
            └── io_timeout

21 directories, 41 files
[root@pve99 ~]$ 
```
## 2.	Між цими вузлами встановлюється мережа.  

## 3.	На одному з вузлів робимо віртуальну машину VSAN на основі Linux таких характеристик (4096Mb, 8Gb) та інсталюємо пакети:  mdadm (збудуємо RAID1 масив з відповідних розділів nvme-дисків вузлів: nvme discavery... , connect... , та потім підтримуємо), tgt (для трансляції цілі md0 iscsi на два вузли), lvm2 (з отриманого  масиву md0 робимо pvcreate, vgcreate), multipath-tools (можна використовувати для покращення продуктивності, коли є можливість зв'язати вузли декількома мережевими з'єднаннями).  

### 3.1. Підготовка віртуальної машини до використання у якості забезпечувача сховища кластера.  

### 3.2. Доберемо Debian в якості операційної системи.Встановлюємо apt install tgt.  

`nano /etc/tgt/conf.d/tgtpve.conf`  

**Наповнення:**  
```
<target iqn.1993-08.org.debian:01:9e746ebec3e>
   backing-store /dev/md0
   initiator-address 10.10.1.1
   initiator-address 10.10.1.2
</target>
```

### 3.3. Встановлюємо lvm2: apt install lvm2  

### 3.4. apt install nvme-cli  

**Завантаження модулів при старті системи:**  

`modprobe nvme_tcp && echo "nvme_tcp" > /etc/modules-load.d/nvme_tcp.conf`  

**Підключитися:**  
```
nvme discover -t tcp -a 10.10.1.1 -s 4420
nvme connect -t tcp -n ora10 -a 10.10.1.1 -s 4420
nvme discover -t tcp -a 10.10.1.2 -s 4420
nvme connect -t tcp -n ora20 -a 10.10.1.2 -s 4420
```
**Щоб підключатися при старті:**
{: .notice--success}

`nano /etc/nvme/discovery.conf`  

**Вставимо:**  
```
# Used for extracting default parameters for discovery
#
# Example:
# --transport=<trtype> --traddr=<traddr> --trsvcid=<trsvcid> --host-traddr=<host-traddr> --host-iface=<host-iface>
discover -t tcp -a 10.10.1.1 -s 4420
discover -t tcp -a 10.10.1.2 -s 4420
Потім:
systemctl enable nvmf-autoconnect.service
```
### 3.3. Підготовка дисків  

**Нода 1**  
```
root@pve1:~# nvme list
Node Generic SN Model Namespace Usage Format FW Rev
/dev/nvme1n1 /dev/ng1n1 S4EUNG0M328258D Samsung SSD 970 EVO Plus 250GB 1 214.99 GB / 250.06 GB 512 B + 0 B 1B2QEXM7
/dev/nvme0n1 /dev/ng0n1 50026B7282A726A4 KINGSTON SKC3000S512G 1 512.11 GB / 512.11 GB 4 KiB + 0 B EIFK31.6
```
**Первіряємо розмір блоку**  
```
root@pve1:~# nvme id-ns /dev/nvme0 -n 1 -H | grep &quot;LBA Format&quot;
[6:5] : 0 Most significant 2 bits of Current LBA Format Selected
[3:0] : 0x1 Least significant 4 bits of Current LBA Format Selected
LBA Format 0 : Metadata Size: 0 bytes - Data Size: 512 bytes - Relative Performance: 0x2 Good
LBA Format 1 : Metadata Size: 0 bytes - Data Size: 4096 bytes - Relative Performance: 0x1 Better (in use)
root@pve1:~#
```
**Міняємо на 4k**  
``` 
root@pve1:~# nvme id-ns /dev/format --lbaf=1 /dev/nvme0n1
```
### 3.4. Розмічаємо диск.
```
root@pve1:~# fdisk /dev/nvme0n1
```
**Записуємо розмітку, щоб скористатися, коли заміняємо диск і зараз для дубля:**  
```
root@pve1:~# sfdisk -d /dev/nvme0n1 > nvmeKINGSTON512.dump
root@pve1:~# cat nvmeKINGSTON512.dump
label: gpt
label-id: A1F37274-73E6-864F-B0B6-9BDD551BBD45
device: /dev/nvme0n1
unit: sectors
first-lba: 256
last-lba: 125026896
sector-size: 4096
/dev/nvme0n1p1 : start= 4096, size= 262144, type=0657FD6D-A4AB-43C4-84E5-0933C84B4F4F, uuid=B21C1B97-64EE-6948-AA0F-0BBA8797EB91
/dev/nvme0n1p2 : start= 266240, size= 104857600, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4, uuid=E71CF74F-B553-5246-A649-3A5C45619225
/dev/nvme0n1p3 : start= 105123840, size= 16777216, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4, uuid=47A78E4D-FC8E-F54B-8DAF-D52A7908590C
/dev/nvme0n1p4 : start= 121901056, size= 3125760, type=0657FD6D-A4AB-43C4-84E5-0933C84B4F4F, uuid=7F23E207-9E1A-1B42-962F-98BED3C1F479
```
**Щоб пізніше відновити цей шаблон, ви можете виконати:**  
```
# sfdisk /dev/nvme0n1 < nvmeKINGSTON512.dump
root@pve1:~#
```
**Другий диск розбиваємо аналогічно**  
```
[root@pve99 ~]$ sfdisk /dev/nvme0n1 < nvmeKINGSTON512.dump
[root@pve99 ~]$ sfdisk -d /dev/nvme0n1 > nvmeKINGSTON512_pve99.dump
[root@pve99 ~]$ cat nvmeKINGSTON512_pve99.dump
label: gpt
label-id: BB0E5F37-05C1-104A-8CB5-6D4A7DA9073A
device: /dev/nvme0n1
unit: sectors
first-lba: 256
last-lba: 125026896
sector-size: 4096
/dev/nvme0n1p1 : start= 4096, size= 262144, type=C12A7328-F81F-11D2-BA4B-00A0C93EC93B, uuid=B139BB2A-C95D-1641-968F-3ED977E40B6A
/dev/nvme0n1p2 : start= 266240, size= 104857600, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4, uuid=3CF0D8B4-A0C3-1145-B923-05D59A56B406
/dev/nvme0n1p3 : start= 105123840, size= 16777216, type=0FC63DAF-8483-4772-8E79-3D69D8477DE4, uuid=35726BB3-0E33-C440-B5A1-14F31EE527FD
/dev/nvme0n1p4 : start= 121901056, size= 3125760, type=0657FD6D-A4AB-43C4-84E5-0933C84B4F4F, uuid=4512150A-2074-5D44-90E3-58826E7A6152
[root@pve99 ~]$
```
## Далі переходимо до віртуальної машини:

### 3.5. Інсталюємо mdadm: `apt install mdadm`

### 3.6. Налагоджуємо Raid1 пристроїв */dev/nvme1n1p2 , /dev/nvme2n1p2*:  
  
# nvme-list  

`root@debvsan:/home/vov# nvme list`    


|   Node     |  Generic    |           SN            |      Model        |  Namespace Usage    |         Format           |       FW      |   Rev     |
|-----------:|:-----------:|:-----------------------:|:-----------------:|:-------------------:|:------------------------:|:-------------:|:----------|
|/dev/nvme2n1|  /dev/ng2n1 |  9782709cba71d57d8e6b   |   pve99-KINGSTON  |           20        |  512.11  GB / 512.11  GB |  4 KiB +  0 B |  6.8.12-5 |
|/dev/nvme1n1|  /dev/ng1n1 |  f1f5c6929bb211d228f4   |   pve1-KINGSTON   |           10        |  512.11  GB / 512.11  GB |  4 KiB +  0 B |  6.8.12-5 |

```
root@debvsan:/home/vov#
# mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/nvme1n1p2 /dev/nvmen1p2
```

